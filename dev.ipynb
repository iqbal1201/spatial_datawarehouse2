{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pipeline.utils.db_conn import db_connection\n",
    "from pipeline.utils.read_sql import read_sql_file\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def db_connection():\n",
    "    try:\n",
    "        # Fetch environment variables\n",
    "        src_database = os.getenv(\"SRC_POSTGRES_DB\")\n",
    "        src_host = os.getenv(\"SRC_POSTGRES_HOST\")\n",
    "        src_user = os.getenv(\"SRC_POSTGRES_USER\")\n",
    "        src_password = os.getenv(\"SRC_POSTGRES_PASSWORD\")\n",
    "        src_port = os.getenv(\"SRC_POSTGRES_PORT\")\n",
    "\n",
    "        dwh_database = os.getenv(\"DWH_POSTGRES_DB\")\n",
    "        dwh_host = os.getenv(\"DWH_POSTGRES_HOST\")\n",
    "        dwh_user = os.getenv(\"DWH_POSTGRES_USER\")\n",
    "        dwh_password = os.getenv(\"DWH_POSTGRES_PASSWORD\")\n",
    "        dwh_port = os.getenv(\"DWH_POSTGRES_PORT\")\n",
    "\n",
    "        # Check if any environment variable is None and print for debugging\n",
    "        if None in [src_database, src_host, src_user, src_password, src_port,\n",
    "                    dwh_database, dwh_host, dwh_user, dwh_password, dwh_port]:\n",
    "            print(\"One or more environment variables are not set.\")\n",
    "            return None\n",
    "\n",
    "        # Create connection strings\n",
    "        src_conn = f'postgresql://{src_user}:{src_password}@{src_host}:{src_port}/{src_database}'\n",
    "        dwh_conn = f'postgresql://{dwh_user}:{dwh_password}@{dwh_host}:{dwh_port}/{dwh_database}'\n",
    "\n",
    "        # Create database engines\n",
    "        src_engine = create_engine(src_conn)\n",
    "        dwh_engine = create_engine(dwh_conn)\n",
    "\n",
    "        return src_engine, dwh_engine\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Using the connection\n",
    "result = db_connection()\n",
    "if result is None:\n",
    "    raise Exception(\"Failed to establish a database connection.\")\n",
    "else:\n",
    "    src_engine, dwh_engine = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# List of required environment variables\n",
    "required_env_vars = [\n",
    "    \"SRC_POSTGRES_DB\",\n",
    "    \"SRC_POSTGRES_HOST\",\n",
    "    \"SRC_POSTGRES_USER\",\n",
    "    \"SRC_POSTGRES_PASSWORD\",\n",
    "    \"SRC_POSTGRES_PORT\",\n",
    "    \"DWH_POSTGRES_DB\",\n",
    "    \"DWH_POSTGRES_HOST\",\n",
    "    \"DWH_POSTGRES_USER\",\n",
    "    \"DWH_POSTGRES_PASSWORD\",\n",
    "    \"DWH_POSTGRES_PORT\"\n",
    "]\n",
    "\n",
    "# Check each variable and print its value or indicate if it's not set\n",
    "for var in required_env_vars:\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        print(f\"{var} is not set.\")\n",
    "    else:\n",
    "        print(f\"{var} is set to: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.utils.read_sql import read_sql_file\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "tables_to_extract = ['public.bangunan']\n",
    "\n",
    "DIR_TEMP_DATA = os.getenv(\"DIR_TEMP_DATA\")\n",
    "DIR_EXTRACT_QUERY = os.getenv(\"DIR_EXTRACT_QUERY\")\n",
    "        \n",
    "# Define db connection engine\n",
    "src_engine, _ = db_connection()\n",
    "\n",
    "# Define the query using the SQL content\n",
    "extract_query = read_sql_file(\n",
    "    file_path = f'{DIR_EXTRACT_QUERY}/all-tables.sql'\n",
    ")\n",
    "\n",
    "\n",
    "for index, table_name in enumerate(tables_to_extract):\n",
    "\n",
    "    # Read data into DataFrame\n",
    "    df = gpd.read_postgis(extract_query.format(table_name = table_name), src_engine, geom_col='geom')\n",
    "\n",
    "    # Write DataFrame to CSV\n",
    "    df.to_file(f\"{DIR_TEMP_DATA}/{table_name}.geojson\", index=False, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "r'D:\\Self_project\\Data_warehousing\\script\\elt_dwh\\pipeline\\src_query\\extract\\all-tables.sql'\n",
    "\n",
    "sql_file_path = r'D:\\Self_project\\Data_warehousing\\script\\elt_dwh\\pipeline\\src_query\\extract\\all-tables.sql'\n",
    "if not os.path.exists(sql_file_path):\n",
    "    raise FileNotFoundError(f\"The SQL file does not exist: {sql_file_path}\")\n",
    "\n",
    "extract_query = read_sql_file(sql_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pipeline.utils.db_conn import db_connection\n",
    "from pipeline.utils.read_sql import read_sql_file\n",
    "import warnings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "    \n",
    "# Define DIR\n",
    "DIR_TEMP_DATA = os.getenv(\"DIR_TEMP_DATA\")\n",
    "DIR_EXTRACT_QUERY = os.getenv(\"DIR_EXTRACT_QUERY\")\n",
    "\n",
    "\n",
    "    \n",
    "# Define tables to be extracted from db sources\n",
    "tables_to_extract = ['public.bangunan']\n",
    "\n",
    "# Define db connection engine\n",
    "src_engine, _ = db_connection()\n",
    "\n",
    "# Define the query using the SQL content\n",
    "extract_query = read_sql_file(\n",
    "    file_path = os.path.join(DIR_EXTRACT_QUERY, 'all-tables.sql')\n",
    ")\n",
    "\n",
    "\n",
    "for index, table_name in enumerate(tables_to_extract):\n",
    "\n",
    "    # Read data into DataFrame\n",
    "    df = gpd.read_postgis(extract_query.format(table_name = table_name), src_engine, geom_col='geom')\n",
    "\n",
    "    # Write DataFrame to CSV\n",
    "    df.to_file(f\"{DIR_TEMP_DATA}/{table_name}.geojson\", index=False, driver='GeoJSON')\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sukses\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pipeline.utils.db_conn import db_connection\n",
    "from pipeline.utils.read_sql import read_sql_file\n",
    "import warnings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def extract():\n",
    "    \n",
    "    # Define DIR\n",
    "    DIR_TEMP_DATA = os.getenv(\"DIR_TEMP_DATA\")\n",
    "    DIR_EXTRACT_QUERY = os.getenv(\"DIR_EXTRACT_QUERY\")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Define tables to be extracted from db sources\n",
    "        tables_to_extract = ['public.bangunan']\n",
    "        \n",
    "        # Define db connection engine\n",
    "        src_engine, _ = db_connection()\n",
    "        \n",
    "        # Define the query using the SQL content\n",
    "        extract_query = read_sql_file(\n",
    "            file_path = f'{DIR_EXTRACT_QUERY}/all-tables.sql'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        for index, table_name in enumerate(tables_to_extract):\n",
    "\n",
    "            # Read data into DataFrame\n",
    "            df = gpd.read_postgis(extract_query.format(table_name = table_name), src_engine, geom_col='geom')\n",
    "\n",
    "            # Write DataFrame to CSV\n",
    "            df.to_file(f\"{DIR_TEMP_DATA}/{table_name}.geojson\", index=False, driver='GeoJSON')\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract data: {e}\")\n",
    "        \n",
    "# Execute the functions when the script is run\n",
    "if __name__ == \"__main__\":\n",
    "    extract()\n",
    "    print(\"Sukses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database engine: Engine(postgresql://postgres:***@localhost:5432/source)\n",
      "Start ELT\n",
      "ELT Success\n"
     ]
    }
   ],
   "source": [
    "from pipeline.extract import *\n",
    "from pipeline.load import *\n",
    "from pipeline.transform import *\n",
    "from pipeline.utils.db_conn import *\n",
    "\n",
    "# Execute the functions when the script is run\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        src_engine, _ = db_connection()\n",
    "        print(f\"Database engine: {src_engine}\")  # This should not be None\n",
    "\n",
    "        print(\"Start ELT\")\n",
    "        extract()\n",
    "        # load()\n",
    "        # transform()\n",
    "\n",
    "        print(\"ELT Success\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error {e}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\dwh\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
